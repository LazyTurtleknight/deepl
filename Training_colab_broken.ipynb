{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LazyTurtleknight/deepl/blob/accelerate_performance/Training_colab_broken.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas\n",
        "!pip install albumentations\n",
        "!pip install torchmetrics\n",
        "!pip install tensorboard"
      ],
      "metadata": {
        "id": "gunIv2fmvQ32",
        "outputId": "b7e7c152-9be1-4282-cec0-461c63ecf10b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.16)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.6)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.11.4)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.1)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.8.1.78)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (4.5.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2023.9.26)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.10.0 torchmetrics-1.2.1\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.59.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9wA2nYwf0GiM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import albumentations as alb\n",
        "#import tensorflow as tf\n",
        "import datetime\n",
        "import torch\n",
        "import numpy as np\n",
        "import opendatasets as od\n",
        "import tensorboard\n",
        "\n",
        "#from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from torchmetrics.classification import Dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k8bZhJ17ESuj",
        "outputId": "a8576ee7-c9d4-4a94-c0a9-c32be32aea89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: simondr\n",
            "Your Kaggle Key: ··········\n",
            "Downloading deepglobe-land-cover-classification-dataset.zip to ./deepglobe-land-cover-classification-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.74G/2.74G [00:21<00:00, 139MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Download dataset from kaggle\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/balraj98/deepglobe-land-cover-classification-dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kLXZPYooESuj"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "# U-Net\n",
        "\n",
        "# Two convolution block. Performs two consecutive convolutions\n",
        "class TwoConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding='same', batch_norm=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.module_list = nn.ModuleList([])\n",
        "\n",
        "        #Using Henriks convultion layering or the one introduced in the Unet paper?\n",
        "        module = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        #torch.nn.init.kaiming_uniform_(module.weight, a=0, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "        self.module_list.append(module)\n",
        "        if batch_norm:\n",
        "            self.module_list.append(nn.BatchNorm2d(in_channels))\n",
        "        self.module_list.append(nn.ReLU())\n",
        "\n",
        "        module = nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding)\n",
        "        #torch.nn.init.kaiming_uniform_(module.weight, a=0, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "        self.module_list.append(module)\n",
        "        if batch_norm:\n",
        "            self.module_list.append(nn.BatchNorm2d(out_channels))\n",
        "        self.module_list.append(nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x\n",
        "        for module in self.module_list:\n",
        "            y = module(y)\n",
        "        return y\n",
        "\n",
        "# UNet encoder block. Performs two convolutions and max pooling.\n",
        "class ConvPool(TwoConv):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding='same', batch_norm=False):\n",
        "        super().__init__(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, batch_norm=batch_norm)\n",
        "        self.max = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        c = super().forward(x)\n",
        "        p = self.max(c)\n",
        "        return c, p\n",
        "\n",
        "# UNet decoder block. Performs upsampling, concatenation of the two inputs and two convolutions.\n",
        "class UpConv(TwoConv):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding='same', batch_norm=False):\n",
        "        super().__init__(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, batch_norm=batch_norm)\n",
        "        # We may use different upsampling method here.\n",
        "        self.upsampling = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        u = self.upsampling(x)\n",
        "        u = torch.cat([u, skip], 1)\n",
        "        c = super().forward(u)\n",
        "        return c, u\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, min, max, num_classes, batch_norm=False):\n",
        "        super().__init__()\n",
        "        self.enc_layers = nn.ModuleList([])\n",
        "        self.dec_layers = nn.ModuleList([])\n",
        "        self.enc_final = None\n",
        "        self.dec_final = None\n",
        "        self.softmax = None\n",
        "\n",
        "        # When go down the encoder/up the decoder the number of filter doubles/halves\n",
        "        # respectively. For that we will generate the powers of two.\n",
        "        # List of powers of 2 [min, 2*min, 4*min, ..., max]\n",
        "        channels = []\n",
        "        power = min\n",
        "        for i in range(int(np.log2(max // min))):\n",
        "            channels.append(power)\n",
        "            power = power*2\n",
        "\n",
        "        # Construct list of blocks for the encoder\n",
        "        self.enc_layers.append(ConvPool(in_channels, min, batch_norm))\n",
        "        for i in range(len(channels)-1):\n",
        "            enc_layer = ConvPool(channels[i], channels[i+1], batch_norm)\n",
        "            self.enc_layers.append(enc_layer)\n",
        "\n",
        "        # Construct list of blocks for the encoder\n",
        "        for i in range(len(channels)-1):\n",
        "            dec_layer = UpConv(channels[i+1], channels[i], batch_norm)\n",
        "            self.dec_layers.insert(0, dec_layer)\n",
        "        self.dec_layers.insert(0, UpConv(max, channels[-1], batch_norm))\n",
        "\n",
        "        # Set up final convolutions for the encoder and decoder\n",
        "        self.enc_final = TwoConv(channels[len(channels)-1], max, 3, 1, 'same')\n",
        "        self.dec_final = nn.Conv2d(min, num_classes, 1, 1)\n",
        "        self.softmax = nn.Softmax(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Collect the values for skip connections to the decoder\n",
        "        skip_connections = []\n",
        "        p = x\n",
        "        # Encoder\n",
        "        for layer in self.enc_layers:\n",
        "            c, p = layer(p)\n",
        "            skip_connections.append(c)\n",
        "\n",
        "        # Bottleneck\n",
        "        c =  self.enc_final(p)\n",
        "\n",
        "        # Decoder\n",
        "        for layer in self.dec_layers:\n",
        "            skip = skip_connections.pop()\n",
        "            c, u = layer(c, skip) # if we do not need c we can use _ instead\n",
        "        c = self.dec_final(c)\n",
        "\n",
        "        return self.softmax(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5ITJlGz0ESuk"
      },
      "outputs": [],
      "source": [
        "# dataset.py\n",
        "#Custom dataset for SatelliteSet\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "# Custom dataset class to load deep globe dataset\n",
        "class SatelliteSet(Dataset):\n",
        "    def __init__(self,\n",
        "                 # mpandas dataframe loaded with a meta data csv containing image file names\n",
        "                 meta_data,\n",
        "                 # Class dictionary\n",
        "                 class_dict,\n",
        "                 # directory where the data is stored\n",
        "                 data_dir,\n",
        "                 # albumentations transform\n",
        "                 transform=None,\n",
        "                 quadrantify=False):\n",
        "        self.meta_data = meta_data\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.class_dict = class_dict\n",
        "        self.quadrantify = quadrantify\n",
        "\n",
        "        \"\"\"\n",
        "         Note:\n",
        "                - We load data completely into main memory and into gpu memory (half of the samples are on cuda, the other on main)\n",
        "                - samples are cropped -> no random crop each epoch\n",
        "                - idx_to_divice keeps track of where samples are (cuda or cpu)\n",
        "        \"\"\"\n",
        "        self.idx_to_device = dict()\n",
        "        # shape: (len(self) // 2) x 3 x 256 x 256\n",
        "        self.images_cpu = None\n",
        "        # shape: (len(self) // 2) x 7 x 256 x 256\n",
        "        self.images_cuda = None\n",
        "        # shape: (len(self) // 2) x 3 x 256 x 256\n",
        "        self.masks_cpu = None\n",
        "        # shape: (len(self) // 2) x 7 x 256 x 256\n",
        "        self.masks_cuda = None\n",
        "        self.prefetch()\n",
        "\n",
        "\n",
        "    def print_memory(self):\n",
        "        func = lambda t: print(t.element_size() * t.nelement())\n",
        "        func(self.images_cpu)\n",
        "        func(self.images_cuda)\n",
        "        func(self.masks_cpu)\n",
        "        func(self.masks_cuda)\n",
        "\n",
        "    # number of samples in dataset\n",
        "    def __len__(self):\n",
        "        return len(self.meta_data) * 4 if self.quadrantify else len(self.meta_data)\n",
        "\n",
        "    def prefetch(self):\n",
        "        toggle = True\n",
        "        for idx in range(len(self.meta_data) // 2):\n",
        "            # Load sample\n",
        "            image, mask = self.load_sample(idx)\n",
        "            n_rows = image.shape[0]\n",
        "            n_cols = image.shape[1]\n",
        "\n",
        "            if self.quadrantify:\n",
        "                self.append_image(image[:n_rows//2, :n_cols//2], mask[:n_rows//2, :n_cols//2], idx*4, toggle)\n",
        "                self.append_image(image[:n_rows//2, n_cols//2:], mask[:n_rows//2, n_cols//2:], idx*4+1, toggle)\n",
        "                self.append_image(image[n_rows//2:, :n_cols//2], mask[n_rows//2:, :n_cols//2], idx*4+2, toggle)\n",
        "                self.append_image(image[n_rows//2:, n_cols//2:], mask[n_rows//2:, n_cols//2:], idx*4+3, toggle)\n",
        "            else:\n",
        "                self.append_image(image, mask, idx, toggle)\n",
        "\n",
        "            toggle = not toggle\n",
        "            if idx % 50 == 1:\n",
        "                print(f\"cpu = {self.images_cuda.shape[0]} and cuda = {self.images_cpu.shape[0]}\")\n",
        "                self.print_memory()\n",
        "\n",
        "    def append_image(self, image, mask, idx, toggle):\n",
        "        image, mask = torch.from_numpy(image), torch.from_numpy(mask)\n",
        "        if toggle:\n",
        "            if self.images_cpu == None:\n",
        "                self.images_cpu = image.unsqueeze(0).cpu()\n",
        "                self.masks_cpu = mask.unsqueeze(0).cpu()\n",
        "            else:\n",
        "                self.images_cpu = torch.cat((self.images_cpu, image.unsqueeze(0).cpu()), dim=0)\n",
        "                self.masks_cpu = torch.cat((self.masks_cpu, mask.unsqueeze(0).cpu()), dim=0)\n",
        "            self.idx_to_device[idx] = ('cpu', self.images_cpu.shape[0]-1)\n",
        "        else:\n",
        "            if self.images_cuda == None:\n",
        "                self.images_cuda = image.unsqueeze(0).cuda()\n",
        "                self.masks_cuda = mask.unsqueeze(0).cuda()\n",
        "            else:\n",
        "                self.images_cuda = torch.cat((self.images_cuda, image.unsqueeze(0).cuda()), dim=0)\n",
        "                self.masks_cuda = torch.cat((self.masks_cuda, mask.unsqueeze(0).cuda()), dim=0)\n",
        "            self.idx_to_device[idx] = ('cuda', self.images_cuda.shape[0]-1)\n",
        "\n",
        "    def transform_sample(self, image, mask):\n",
        "        # In the transform from albumentation we pass both the image and the mask together to make sure\n",
        "        # they undergo the same transformation, e.g. this ensure both have the same random crop\n",
        "        transformed = self.transform(image = image, mask = mask)\n",
        "        image = transformed['image'].to(torch.float32)\n",
        "        mask = transformed['mask']\n",
        "        # Mask will be a one hot encoded matrix with dimensions 7 x 256 x 256\n",
        "        # First dimenion corresponds to class label\n",
        "        mask = np.apply_along_axis(lambda k: self.class_dict[tuple(k)], 2, mask)\n",
        "        mask = (mask[..., None] == np.arange(7)).astype(mask.dtype)\n",
        "        mask = np.transpose(mask, (2, 0, 1))\n",
        "        mask = torch.tensor(mask).to(torch.float32)\n",
        "        return image, mask\n",
        "\n",
        "    def load_sample(self, idx):\n",
        "            # Read image\n",
        "            img_path = os.path.join(self.data_dir, self.meta_data.iloc[idx]['sat_image_path'])\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            # Read target mask\n",
        "            mask_path = os.path.join(self.data_dir, self.meta_data.iloc[idx]['mask_path'])\n",
        "            mask = cv2.imread(mask_path)\n",
        "            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            return image, mask\n",
        "    def __getitem__(self, idx):\n",
        "        image, mask = 0, 0\n",
        "        if idx not in self.idx_to_device:\n",
        "            image, mask = self.load_sample(idx)\n",
        "        else:\n",
        "            device, index = self.idx_to_device[idx]\n",
        "            if device == 'cuda':\n",
        "                image, mask = self.images_cuda[index].cpu(), self.masks_cuda[index].cpu()\n",
        "            else:\n",
        "                image, mask = self.images_cpu[index], self.masks_cpu[index]\n",
        "            image, mask = image.numpy(), mask.numpy()\n",
        "        image, mask = self.transform_sample(image, mask)\n",
        "        image, mask = image.cuda(), mask.cuda()\n",
        "        return image, mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_P8xHWCpESuk"
      },
      "outputs": [],
      "source": [
        "# utils.py\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torchmetrics import JaccardIndex\n",
        "from torchmetrics.classification import Dice\n",
        "\n",
        "def plot(sample, data_dir):\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    ax = plt.subplot(2,2,1)\n",
        "    plt.imshow(np.asarray(Image.open(os.path.join(data_dir, sample['sat_image_path'].iloc[0]))))\n",
        "    plt.gray()\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "\n",
        "    ax = plt.subplot(2,2,2)\n",
        "    plt.imshow(np.asarray(Image.open(os.path.join(data_dir, sample['mask_path'].iloc[0]))))\n",
        "    plt.gray()\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# TODO: make something like a dictionary for parametere to pass to data loader\n",
        "def get_data_loaders(data_dir, transform, shuffle_dataset, test_split, random_seed, batch_size, quadrantify):\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    # Load metadata csv\n",
        "    metadata = pd.read_csv(os.path.join(data_dir, 'metadata.csv'))\n",
        "\n",
        "    # We need to filter for row where 'split' is 'train' because samples where\n",
        "    # 'split' is 'valid' or 'test' have no target mask\n",
        "    metadata = metadata[metadata['split'] == 'train']\n",
        "\n",
        "    class_dict = pd.read_csv(os.path.join(data_dir, 'class_dict.csv'))\n",
        "\n",
        "    classes = {}\n",
        "    c = 0\n",
        "    for i in class_dict.index:\n",
        "        classes[tuple(class_dict.iloc[i,1:])] = c\n",
        "        c += 1\n",
        "\n",
        "    # We need to filter for row where 'split' is 'train' because samples where\n",
        "    # 'split' is 'valid' or 'test' have no target mask\n",
        "\n",
        "    dataset = SatelliteSet(meta_data=metadata, class_dict=classes, data_dir=data_dir, transform=transform, quadrantify=quadrantify)\n",
        "\n",
        "    # Creating data indices for training and validation splits:\n",
        "    dataset_size = len(dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "    split = int(np.floor(test_split * dataset_size))\n",
        "    #TODO:Implement validation\n",
        "\n",
        "    if shuffle_dataset:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "    train_indices, test_indices = indices[split:], indices[:split]\n",
        "\n",
        "    # Creating PT data samplers and loaders:\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                            sampler=train_sampler)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                                sampler=test_sampler)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def evaluate(model, writer, dataloader, epoch, device):\n",
        "    iou_score = 0\n",
        "    iou = JaccardIndex('multiclass', num_classes=7).to(device)\n",
        "    dice_score = 0\n",
        "    dice = Dice(num_classes=7).to(device)\n",
        "    for x, y in dataloader:\n",
        "        pred = model(x)\n",
        "        pred = torch.argmax(pred, 1)\n",
        "        y = torch.argmax(y, 1)\n",
        "        # Calculate IOU\n",
        "        iou_score += iou(pred, y)\n",
        "        # Calculate DICE\n",
        "        dice_score += dice(pred, y)\n",
        "\n",
        "    writer.add_scalar('DICE Score', dice_score / len(dataloader), epoch)\n",
        "    writer.add_scalar('IOU Score', iou_score / len(dataloader), epoch)\n",
        "\n",
        "def get_iou_score(pred, y):\n",
        "    iou_score = 0\n",
        "    intersection = np.logical_and(y, pred)\n",
        "    union = np.logical_or(y, pred)\n",
        "    iou_score += np.sum(intersection) / np.sum(union)\n",
        "    return iou_score\n",
        "\n",
        "def get_dice_score(pred, y):\n",
        "    dice_score = 0\n",
        "    intersection = np.logical_and(y, pred)\n",
        "    dice_score += np.sum(intersection) / (y.size() + pred.size())\n",
        "    return dice_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pRH03Q1AyFAc"
      },
      "outputs": [],
      "source": [
        "# Define constants\n",
        "\n",
        "# directories\n",
        "data_dir = 'deepglobe-land-cover-classification-dataset' # change to directory containing the data\n",
        "trained_models = 'trained_models'\n",
        "train_dir = 'train'\n",
        "log_dir = 'runs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SuBN7hd2ESul"
      },
      "outputs": [],
      "source": [
        "# Training configuration (hyperparameters)\n",
        "\n",
        "## Data\n",
        "test_split = .2 #20% for test split\n",
        "#valdation_split = .2 #20% for validation split\n",
        "random_seed = np.random.seed()\n",
        "shuffle_dataset = True\n",
        "quadrantify = False\n",
        "\n",
        "transform = alb.Compose([\n",
        "    alb.RandomCrop(width=256, height=256),\n",
        "    alb.HorizontalFlip(p=0.5),\n",
        "    ToTensorV2()\n",
        "    ],\n",
        "    # we want the mask and the image to have the same augmentation (especially when we crop)\n",
        "    # this way we pass the image and the mask simultaneously to the pipeline\n",
        "    additional_targets={'image': 'image', 'mask': 'mask'}\n",
        "    )\n",
        "\n",
        "## model architecture\n",
        "in_channels = 3\n",
        "min_channels = 16\n",
        "max_channels = 128\n",
        "num_classes = 7\n",
        "\n",
        "## Training\n",
        "learning_rate = 0.1\n",
        "batch_size = 20\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init data loader/generator\n",
        "train_dataloader, test_dataloader = get_data_loaders(data_dir, transform, shuffle_dataset, test_split, random_seed, batch_size, quadrantify)\n"
      ],
      "metadata": {
        "id": "rLxQsZi5Zv8G",
        "outputId": "72fd623a-5840-41e2-82cb-df2a086fb6ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu = 1 and cuda = 1\n",
            "17978112\n",
            "17978112\n",
            "17978112\n",
            "17978112\n",
            "cpu = 26 and cuda = 26\n",
            "467430912\n",
            "467430912\n",
            "467430912\n",
            "467430912\n",
            "cpu = 51 and cuda = 51\n",
            "916883712\n",
            "916883712\n",
            "916883712\n",
            "916883712\n",
            "cpu = 76 and cuda = 76\n",
            "1366336512\n",
            "1366336512\n",
            "1366336512\n",
            "1366336512\n",
            "cpu = 101 and cuda = 101\n",
            "1815789312\n",
            "1815789312\n",
            "1815789312\n",
            "1815789312\n",
            "cpu = 126 and cuda = 126\n",
            "2265242112\n",
            "2265242112\n",
            "2265242112\n",
            "2265242112\n",
            "cpu = 151 and cuda = 151\n",
            "2714694912\n",
            "2714694912\n",
            "2714694912\n",
            "2714694912\n",
            "cpu = 176 and cuda = 176\n",
            "3164147712\n",
            "3164147712\n",
            "3164147712\n",
            "3164147712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CGs0fakYloJp",
        "outputId": "ece5aefe-2e14-420d-e03d-40260f59ff5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Batch number 1 of 33\n",
            "same\n",
            "zeros\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-fb2d0c53e097>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch number {c} of {len(train_dataloader)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# pred has shape batch_size x num_classes x width x height\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# apply softmax to find what it thinks is the most likely class label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a84aa2221c95>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;31m# Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0mskip_connections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a84aa2221c95>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a84aa2221c95>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: negative padding is not supported"
          ]
        }
      ],
      "source": [
        "from sys import breakpointhook\n",
        "# Training\n",
        "\n",
        "# setup training enviroment\n",
        "device = torch.device(\"cuda\")\n",
        "#Labels\n",
        "\n",
        "\n",
        "# init model, optimizer and loss function\n",
        "model = UNet(in_channels, min_channels, max_channels, num_classes)\n",
        "model.to(device)\n",
        "opt = torch.optim.SGD(model.parameters(), learning_rate)\n",
        "loss_func = torch.nn.CrossEntropyLoss() #Dice(num_classes=num_classes)\n",
        "\n",
        "# Set up summary writer for tensorboard\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "writer = SummaryWriter(os.path.join(log_dir, current_time))\n",
        "\n",
        "# start training\n",
        "for epoch in range(1, epochs+1):\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "    c = 1\n",
        "    loss_avg = 0\n",
        "    for x, y in train_dataloader:\n",
        "        print(f\"Batch number {c} of {len(train_dataloader)}\")\n",
        "        opt.zero_grad()\n",
        "        pred = model(x)\n",
        "        # pred has shape batch_size x num_classes x width x height\n",
        "        # apply softmax to find what it thinks is the most likely class label\n",
        "        pred = torch.softmax(pred, 1)\n",
        "        # y has shape batch_size x num_classes x width x height\n",
        "        # the num_classes is a one hot encoding where the index corresponds to a class\n",
        "        # which can be looked up in the class_dict\n",
        "        loss = loss_func(pred, y)\n",
        "        loss_avg += loss\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        c += 1\n",
        "\n",
        "    writer.add_scalar('Loss', loss_avg, epoch)\n",
        "\n",
        "    # Evaluate model\n",
        "    model.eval()\n",
        "    evaluate(model, writer, test_dataloader, epoch, device)\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8ltRRM3ESul"
      },
      "outputs": [],
      "source": [
        "# Launch tensorboard\n",
        "%load_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir runs/20231206-080048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1pRWV_VESul"
      },
      "outputs": [],
      "source": [
        "# List running tensorboard instances\n",
        "tensorboard.notebook.list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWtgf1hzESum"
      },
      "outputs": [],
      "source": [
        "# kill process with id\n",
        "!kill -9 27428"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uf_DDyHESum"
      },
      "outputs": [],
      "source": [
        "# Save model after training\n",
        "torch.save(model.state_dict(), os.path.join(trained_models, current_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOJqYFFZESum"
      },
      "outputs": [],
      "source": [
        "# Model evaluation\n",
        "\n",
        "model = UNet(0, 0, 0, 0)\n",
        "model.load_state_dict(torch.load(PATH_To_Saved_Model))\n",
        "model.eval()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}