{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas\n",
        "!pip install albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9wA2nYwf0GiM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Prgm\\Anaconda\\envs\\deepl\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] Die angegebene Prozedur wurde nicht gefunden'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import albumentations as alb\n",
        "#import tensorflow as tf\n",
        "import datetime\n",
        "import torch\n",
        "import numpy as np\n",
        "import opendatasets as od\n",
        "\n",
        "from model import UNet\n",
        "from utils import plot, get_data_loaders, evaluate, get_dice_score\n",
        "#from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from torchmetrics.classification import Dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset from kaggle\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/balraj98/deepglobe-land-cover-classification-dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model.py\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "# U-Net\n",
        "\n",
        "# Two convolution block. Performs two consecutive convolutions\n",
        "class TwoConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding='same'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.module_list = nn.ModuleList([])\n",
        "        \n",
        "        #Using Henriks convultion layering or the one introduced in the Unet paper?\n",
        "        self.module_list.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n",
        "        self.module_list.append(nn.ReLU())\n",
        "\n",
        "        self.module_list.append(nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding))\n",
        "        self.module_list.append(nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x\n",
        "        for module in self.module_list:\n",
        "            y = module(y)\n",
        "        return y\n",
        "\n",
        "# UNet encoder block. Performs two convolutions and max pooling.\n",
        "class ConvPool(TwoConv):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding='same'):\n",
        "        super().__init__(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.max = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        c = super().forward(x)\n",
        "        p = self.max(c)\n",
        "        return c, p\n",
        "\n",
        "# UNet decoder block. Performs upsampling, concatenation of the two inputs and two convolutions.\n",
        "class UpConv(TwoConv):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding='same'):\n",
        "        super().__init__(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        # We may use different upsampling method here.\n",
        "        self.upsampling = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        u = self.upsampling(x)\n",
        "        u = torch.cat([u, skip], 1)\n",
        "        c = super().forward(u)\n",
        "        return c, u\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, min, max, num_classes):\n",
        "        super().__init__()\n",
        "        self.enc_layers = nn.ModuleList([])\n",
        "        self.dec_layers = nn.ModuleList([])\n",
        "        self.enc_final = None\n",
        "        self.dec_final = None\n",
        "        self.softmax = None\n",
        "\n",
        "        # When go down the encoder/up the decoder the number of filter doubles/halves\n",
        "        # respectively. For that we will generate the powers of two.\n",
        "        # List of powers of 2 [min, 2*min, 4*min, ..., max]\n",
        "        channels = []\n",
        "        power = min\n",
        "        for i in range(int(np.log2(max // min))):\n",
        "            channels.append(power)\n",
        "            power = power*2\n",
        "\n",
        "        # Construct list of blocks for the encoder\n",
        "        self.enc_layers.append(ConvPool(in_channels, min))\n",
        "        for i in range(len(channels)-1):\n",
        "            enc_layer = ConvPool(channels[i], channels[i+1])\n",
        "            self.enc_layers.append(enc_layer)\n",
        "\n",
        "        # Construct list of blocks for the encoder\n",
        "        for i in range(len(channels)-1):\n",
        "            dec_layer = UpConv(channels[i+1], channels[i])\n",
        "            self.dec_layers.insert(0, dec_layer)\n",
        "        self.dec_layers.insert(0, UpConv(max, channels[-1]))\n",
        "\n",
        "        # Set up final convolutions for the encoder and decoder\n",
        "        self.enc_final = TwoConv(channels[len(channels)-1], max, 3, 1, 'same')\n",
        "        self.dec_final = nn.Conv2d(min, num_classes, 1, 1)\n",
        "        self.softmax = nn.Softmax(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Collect the values for skip connections to the decoder\n",
        "        skip_connections = []\n",
        "        p = x\n",
        "        # Encoder\n",
        "        for layer in self.enc_layers:\n",
        "            c, p = layer(p)\n",
        "            skip_connections.append(c)\n",
        "\n",
        "        # Bottleneck\n",
        "        c =  self.enc_final(p)\n",
        "\n",
        "        # Decoder\n",
        "        for layer in self.dec_layers:\n",
        "            skip = skip_connections.pop()\n",
        "            c, u = layer(c, skip) # if we do not need c we can use _ instead\n",
        "        c = self.dec_final(c)\n",
        "\n",
        "        return self.softmax(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataset.py\n",
        "#Custom dataset for SatelliteSet\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "# Custom dataset class to load deep globe dataset\n",
        "class SatelliteSet(Dataset):\n",
        "    def __init__(self,\n",
        "                 # mpandas dataframe loaded with a meta data csv containing image file names\n",
        "                 meta_data, \n",
        "                 # Class dictionary\n",
        "                 class_dict,\n",
        "                 # directory where the data is stored\n",
        "                 data_dir, \n",
        "                 # albumentations transform\n",
        "                 transform=None):\n",
        "        self.meta_data = meta_data\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.class_dict = class_dict\n",
        "\n",
        "    # number of samples in dataset\n",
        "    def __len__(self):\n",
        "        return len(self.meta_data)\n",
        "\n",
        "    # load and return sample at index idx\n",
        "    def __getitem__(self, idx):\n",
        "        # Read image\n",
        "        img_path = os.path.join(self.data_dir, self.meta_data.iloc[idx]['sat_image_path'])\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        # Read target mask\n",
        "        mask_path = os.path.join(self.data_dir, self.meta_data.iloc[idx]['mask_path'])\n",
        "        mask = cv2.imread(mask_path)\n",
        "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
        "        if self.transform:\n",
        "            # In the transform from albumentation we pass both the image and the mask together to make sure\n",
        "            # they undergo the same transformation, e.g. this ensure both have the same random crop\n",
        "            transformed = self.transform(image = image, mask = mask)\n",
        "            image = transformed['image'].to(torch.float32)\n",
        "            mask = transformed['mask']\n",
        "            mask = torch.tensor(np.apply_along_axis(lambda k: self.class_dict[tuple(k)], 2, mask))\n",
        "            mask_onehot = torch.zeros((len(self.class_dict), mask.shape[0], mask.shape[1]))\n",
        "            for w,h in mask.nonzero(as_tuple=False):\n",
        "                mask_onehot[mask[w,h], w, h] = 1\n",
        "            mask = mask_onehot.to(torch.float32)\n",
        "\n",
        "        #image.require_grad = True\n",
        "\n",
        "        return image, mask\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# utils.py\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from PIL import Image\n",
        "from dataset import SatelliteSet\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torchmetrics import JaccardIndex\n",
        "from torchmetrics.classification import Dice\n",
        "\n",
        "def plot(sample, data_dir):\n",
        "\n",
        "    plt.figure(figsize=(5,4))\n",
        "    ax = plt.subplot(2,2,1)\n",
        "    plt.imshow(np.asarray(Image.open(os.path.join(data_dir, sample['sat_image_path'].iloc[0]))))\n",
        "    plt.gray()\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "\n",
        "    ax = plt.subplot(2,2,2)\n",
        "    plt.imshow(np.asarray(Image.open(os.path.join(data_dir, sample['mask_path'].iloc[0]))))\n",
        "    plt.gray()\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# TODO: make something like a dictionary for parametere to pass to data loader\n",
        "def get_data_loaders(data_dir, transform, shuffle_dataset, test_split, random_seed, batch_size):\n",
        "\n",
        "    # Load metadata csv\n",
        "    metadata = pd.read_csv(os.path.join(data_dir, 'subsample.csv'))\n",
        "\n",
        "    # We need to filter for row where 'split' is 'train' because samples where\n",
        "    # 'split' is 'valid' or 'test' have no target mask\n",
        "    metadata = metadata[metadata['split'] == 'train']\n",
        "\n",
        "    class_dict = pd.read_csv(os.path.join(data_dir, 'class_dict.csv'))\n",
        "\n",
        "    classes = {}\n",
        "    c = 0\n",
        "    for i in class_dict.index:\n",
        "        classes[tuple(class_dict.iloc[i,1:])] = c\n",
        "        c += 1\n",
        "\n",
        "    # We need to filter for row where 'split' is 'train' because samples where\n",
        "    # 'split' is 'valid' or 'test' have no target mask\n",
        "\n",
        "    dataset = SatelliteSet(meta_data=metadata, class_dict=classes, data_dir=data_dir, transform=transform)\n",
        "\n",
        "    # Creating data indices for training and validation splits:\n",
        "    dataset_size = len(dataset)\n",
        "    indices = list(range(dataset_size))\n",
        "    split = int(np.floor(test_split * dataset_size))\n",
        "    #TODO:Implement validation\n",
        "\n",
        "    if shuffle_dataset:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "    train_indices, test_indices = indices[split:], indices[:split]\n",
        "\n",
        "    # Creating PT data samplers and loaders:\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
        "                                            sampler=train_sampler)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                                sampler=test_sampler)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "def evaluate(model, writer, dataloader, epoch):\n",
        "    iou_score = 0\n",
        "    iou = JaccardIndex('multiclass', num_classes=7)\n",
        "    dice_score = 0\n",
        "    dice = Dice(num_classes=7)\n",
        "    for x, y in dataloader:\n",
        "        pred = model(x)\n",
        "        pred = torch.argmax(pred, 1)\n",
        "        y = torch.argmax(y, 1)\n",
        "        # Calculate IOU\n",
        "        iou_score += iou(pred, y)\n",
        "        # Calculate DICE\n",
        "        dice_score += dice(pred, y)\n",
        "\n",
        "    writer.add_scalar('DICE Score', dice_score / len(dataloader), epoch)\n",
        "    writer.add_scalar('IOU Score', iou_score / len(dataloader), epoch)\n",
        "\n",
        "def get_iou_score(pred, y):\n",
        "    iou_score = 0\n",
        "    intersection = np.logical_and(y, pred)\n",
        "    union = np.logical_or(y, pred)\n",
        "    iou_score += np.sum(intersection) / np.sum(union)\n",
        "    return iou_score\n",
        "\n",
        "def get_dice_score(pred, y):\n",
        "    dice_score = 0\n",
        "    intersection = np.logical_and(y, pred)\n",
        "    dice_score += np.sum(intersection) / (y.size() + pred.size())\n",
        "    return dice_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pRH03Q1AyFAc"
      },
      "outputs": [],
      "source": [
        "# Define constants\n",
        "\n",
        "# directories\n",
        "data_dir = 'deepglobe-land-cover-classification-dataset' # change to directory containing the data\n",
        "trained_models = 'trained_models'\n",
        "train_dir = 'train'\n",
        "log_dir = 'runs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration (hyperparameters)\n",
        "\n",
        "## Data\n",
        "test_split = .2 #20% for test split\n",
        "#valdation_split = .2 #20% for validation split\n",
        "random_seed = np.random.seed()\n",
        "shuffle_dataset = True\n",
        "\n",
        "transform = alb.Compose([\n",
        "    alb.RandomCrop(width=256, height=256),\n",
        "    alb.HorizontalFlip(p=0.5),\n",
        "    ToTensorV2()\n",
        "    ],\n",
        "    # we want the mask and the image to have the same augmentation (especially when we crop)\n",
        "    # this way we pass the image and the mask simultaneously to the pipeline\n",
        "    additional_targets={'image': 'image', 'mask': 'mask'}\n",
        "    )\n",
        "\n",
        "## model architecture\n",
        "in_channels = 3\n",
        "min_channels = 16\n",
        "max_channels = 128\n",
        "num_classes = 7\n",
        "\n",
        "## Training\n",
        "learning_rate = 0.1\n",
        "batch_size = 10\n",
        "epochs = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CGs0fakYloJp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Batch\n",
            "Batch\n",
            "Epoch: 1\n",
            "Batch\n",
            "Batch\n",
            "Epoch: 2\n",
            "Batch\n",
            "Batch\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Parent directory trained_models does not exist.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32md:\\git\\deepl\\Training.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/git/deepl/Training.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/git/deepl/Training.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Save model after training\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/git/deepl/Training.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(trained_models, current_time))\n",
            "File \u001b[1;32md:\\Prgm\\Anaconda\\envs\\deepl\\Lib\\site-packages\\torch\\serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    617\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 618\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[0;32m    620\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
            "File \u001b[1;32md:\\Prgm\\Anaconda\\envs\\deepl\\Lib\\site-packages\\torch\\serialization.py:492\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 492\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
            "File \u001b[1;32md:\\Prgm\\Anaconda\\envs\\deepl\\Lib\\site-packages\\torch\\serialization.py:463\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mPyTorchFileWriter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_stream))\n\u001b[0;32m    462\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mPyTorchFileWriter(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname))\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Parent directory trained_models does not exist."
          ]
        }
      ],
      "source": [
        "# Training\n",
        "\n",
        "# setup training enviroment\n",
        "\n",
        "#Labels\n",
        "\n",
        "\n",
        "# init data loader/generator\n",
        "train_dataloader, test_dataloader = get_data_loaders(data_dir, transform, shuffle_dataset, test_split, random_seed, batch_size)\n",
        "\n",
        "# init model, optimizer and loss function\n",
        "model = UNet(in_channels, min_channels, max_channels, num_classes)\n",
        "opt = torch.optim.SGD(model.parameters(), learning_rate)\n",
        "loss_func = torch.nn.CrossEntropyLoss() #Dice(num_classes=num_classes)\n",
        "\n",
        "# Set up summary writer for tensorboard\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "writer = SummaryWriter(os.path.join(log_dir, current_time))\n",
        "\n",
        "# start training\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch: {epoch}\")\n",
        "    for x, y in train_dataloader:\n",
        "        opt.zero_grad() \n",
        "        pred = model(x)\n",
        "        # TODO: check the output of the model. is it one hot encoded, rgb or else ?\n",
        "        #pred = torch.argmax(pred, 1)\n",
        "        pred = torch.softmax(pred, 1)\n",
        "        #pred.requires_grad = True\n",
        "        loss = loss_func(pred, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        \n",
        "    # TODO: this is only the loss from the last batch. we want the accumulated loss or something else.\n",
        "    writer.add_scalar('Loss', loss, epoch)\n",
        "    \n",
        "    # Evaluate model\n",
        "    model.eval()\n",
        "    evaluate(model, writer, test_dataloader, epoch)\n",
        "    model.train()\n",
        "\n",
        "# Save model after training\n",
        "torch.save(model.state_dict(), os.path.join(trained_models, current_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model evaluation\n",
        "\n",
        "model = UNet(0, 0, 0, 0)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
